{"ast":null,"code":"var _jsxFileName = \"/Users/somk/Documents/GitHub/AweSum-FE/src/components/summary/SummaryModal.js\",\n    _s = $RefreshSig$();\n\nimport React, { useState, useEffect } from 'react';\nimport styled from 'styled-components';\nimport ModalFrame from '../common/ModalFrame';\nimport palette from '../../styles/palette.js';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst Textbox = styled.div`\n  background: ${palette.pink[0]};\n  border-radius: 16px;\n  height: 85%;\n  width: 90%;\n  display: flex;\n  flex-direction: column;\n  justify-content: center;\n  align-items: center;\n  overflow: auto;\n`;\n_c = Textbox;\nconst Text = styled.div`\n  font-family: 'Roboto', sans-serif;\n  font-style: normal;\n  font-weight: normal;\n  font-size: 1.25rem;\n  text-align: center;\n  height: 520px;\n  width: 90%;\n  overflow: auto;\n`;\n_c2 = Text;\n\nconst SummaryModal = _ref => {\n  _s();\n\n  let {\n    length,\n    videoId,\n    _handleModal\n  } = _ref;\n  const [summary, setSummary] = useState('hello');\n  return /*#__PURE__*/_jsxDEV(ModalFrame, {\n    _handleModal: _handleModal,\n    children: [/*#__PURE__*/_jsxDEV(\"h1\", {\n      children: \"Summary\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 34,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(Textbox, {\n      children: [/*#__PURE__*/_jsxDEV(Text, {\n        children: \"The fundamental building block of deep learning is just a single, neuron also known as a perceptron. We want to find the weights of the neural network that will minimize the loss of our data set. If we compute the gradient of our laws, with respect to our weights, that's the derivative our gradient for loss with respect to the weights. Now, let's take a look at what's called a single layered neural network. We want to find the weights of the neural network that will minimize the loss of our data set. We compute the gradient of our loss with respect to each of the weights in our neural network. We want to learn a model that accurately describes our test data, not the training data, even though we're optimizing this model based on the training data. This is a very simple neural network that only has one input, one hidden neuron, and one output. The fundamental building block of deep learning is just a single, neuron also known as a perceptron. We want to find the weights of the neural network that will minimize the loss of our data set. If we compute the gradient of our laws, with respect to our weights, that's the derivative our gradient for loss with respect to the weights. Now, let's take a look at what's called a single layered neural network. We want to find the weights of the neural network that will minimize the loss of our data set. We compute the gradient of our loss with respect to each of the weights in our neural network. We want to learn a model that accurately describes our test data, not the training data, even though we're optimizing this model based on the training data. This is a very simple neural network that only has one input, one hidden neuron, and one output. The fundamental building block of deep learning is just a single, neuron also known as a perceptron. We want to find the weights of the neural network that will minimize the loss of our data set. If we compute the gradient of our laws, with respect to our weights, that's the derivative our gradient for loss with respect to the weights. Now, let's take a look at what's called a single layered neural network. We want to find the weights of the neural network that will minimize the loss of our data set. We compute the gradient of our loss with respect to each of the weights in our neural network. We want to learn a model that accurately describes our test data, not the training data, even though we're optimizing this model based on the training data. This is a very simple neural network that only has one input, one hidden neuron, and one output. The fundamental building block of deep learning is just a single, neuron also known as a perceptron. We want to find the weights of the neural network that will minimize the loss of our data set. If we compute the gradient of our laws, with respect to our weights, that's the derivative our gradient for loss with respect to the weights. Now, let's take a look at what's called a single layered neural network. We want to find the weights of the neural network that will minimize the loss of our data set. We compute the gradient of our loss with respect to each of the weights in our neural network. We want to learn a model that accurately describes our test data, not the training data, even though we're optimizing this model based on the training data. This is a very simple neural network that only has one input, one hidden neuron, and one output.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 36,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(Text, {\n        children: length\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 88,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 33,\n    columnNumber: 5\n  }, this);\n};\n\n_s(SummaryModal, \"k06lNXqYWqcDwqi2zN9eGdYqUWI=\");\n\n_c3 = SummaryModal;\nexport default SummaryModal;\n\nvar _c, _c2, _c3;\n\n$RefreshReg$(_c, \"Textbox\");\n$RefreshReg$(_c2, \"Text\");\n$RefreshReg$(_c3, \"SummaryModal\");","map":{"version":3,"sources":["/Users/somk/Documents/GitHub/AweSum-FE/src/components/summary/SummaryModal.js"],"names":["React","useState","useEffect","styled","ModalFrame","palette","Textbox","div","pink","Text","SummaryModal","length","videoId","_handleModal","summary","setSummary"],"mappings":";;;AAAA,OAAOA,KAAP,IAAgBC,QAAhB,EAA0BC,SAA1B,QAA2C,OAA3C;AACA,OAAOC,MAAP,MAAmB,mBAAnB;AACA,OAAOC,UAAP,MAAuB,sBAAvB;AACA,OAAOC,OAAP,MAAoB,yBAApB;;AAEA,MAAMC,OAAO,GAAGH,MAAM,CAACI,GAAI;AAC3B,gBAAgBF,OAAO,CAACG,IAAR,CAAa,CAAb,CAAgB;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAVA;KAAMF,O;AAYN,MAAMG,IAAI,GAAGN,MAAM,CAACI,GAAI;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CATA;MAAME,I;;AAWN,MAAMC,YAAY,GAAG,QAAuC;AAAA;;AAAA,MAAtC;AAAEC,IAAAA,MAAF;AAAUC,IAAAA,OAAV;AAAmBC,IAAAA;AAAnB,GAAsC;AAC1D,QAAM,CAACC,OAAD,EAAUC,UAAV,IAAwBd,QAAQ,CAAC,OAAD,CAAtC;AAEA,sBACE,QAAC,UAAD;AAAY,IAAA,YAAY,EAAEY,YAA1B;AAAA,4BACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YADF,eAEE,QAAC,OAAD;AAAA,8BACE,QAAC,IAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADF,eAqDE,QAAC,IAAD;AAAA,kBAAOF;AAAP;AAAA;AAAA;AAAA;AAAA,cArDF;AAAA;AAAA;AAAA;AAAA;AAAA,YAFF;AAAA;AAAA;AAAA;AAAA;AAAA,UADF;AA4DD,CA/DD;;GAAMD,Y;;MAAAA,Y;AAiEN,eAAeA,YAAf","sourcesContent":["import React, { useState, useEffect } from 'react';\nimport styled from 'styled-components';\nimport ModalFrame from '../common/ModalFrame';\nimport palette from '../../styles/palette.js';\n\nconst Textbox = styled.div`\n  background: ${palette.pink[0]};\n  border-radius: 16px;\n  height: 85%;\n  width: 90%;\n  display: flex;\n  flex-direction: column;\n  justify-content: center;\n  align-items: center;\n  overflow: auto;\n`;\n\nconst Text = styled.div`\n  font-family: 'Roboto', sans-serif;\n  font-style: normal;\n  font-weight: normal;\n  font-size: 1.25rem;\n  text-align: center;\n  height: 520px;\n  width: 90%;\n  overflow: auto;\n`;\n\nconst SummaryModal = ({ length, videoId, _handleModal }) => {\n  const [summary, setSummary] = useState('hello');\n\n  return (\n    <ModalFrame _handleModal={_handleModal}>\n      <h1>Summary</h1>\n      <Textbox>\n        <Text>\n          The fundamental building block of deep learning is just a single,\n          neuron also known as a perceptron. We want to find the weights of the\n          neural network that will minimize the loss of our data set. If we\n          compute the gradient of our laws, with respect to our weights, that's\n          the derivative our gradient for loss with respect to the weights. Now,\n          let's take a look at what's called a single layered neural network. We\n          want to find the weights of the neural network that will minimize the\n          loss of our data set. We compute the gradient of our loss with respect\n          to each of the weights in our neural network. We want to learn a model\n          that accurately describes our test data, not the training data, even\n          though we're optimizing this model based on the training data. This is\n          a very simple neural network that only has one input, one hidden\n          neuron, and one output. The fundamental building block of deep\n          learning is just a single, neuron also known as a perceptron. We want\n          to find the weights of the neural network that will minimize the loss\n          of our data set. If we compute the gradient of our laws, with respect\n          to our weights, that's the derivative our gradient for loss with\n          respect to the weights. Now, let's take a look at what's called a\n          single layered neural network. We want to find the weights of the\n          neural network that will minimize the loss of our data set. We compute\n          the gradient of our loss with respect to each of the weights in our\n          neural network. We want to learn a model that accurately describes our\n          test data, not the training data, even though we're optimizing this\n          model based on the training data. This is a very simple neural network\n          that only has one input, one hidden neuron, and one output. The\n          fundamental building block of deep learning is just a single, neuron\n          also known as a perceptron. We want to find the weights of the neural\n          network that will minimize the loss of our data set. If we compute the\n          gradient of our laws, with respect to our weights, that's the\n          derivative our gradient for loss with respect to the weights. Now,\n          let's take a look at what's called a single layered neural network. We\n          want to find the weights of the neural network that will minimize the\n          loss of our data set. We compute the gradient of our loss with respect\n          to each of the weights in our neural network. We want to learn a model\n          that accurately describes our test data, not the training data, even\n          though we're optimizing this model based on the training data. This is\n          a very simple neural network that only has one input, one hidden\n          neuron, and one output. The fundamental building block of deep\n          learning is just a single, neuron also known as a perceptron. We want\n          to find the weights of the neural network that will minimize the loss\n          of our data set. If we compute the gradient of our laws, with respect\n          to our weights, that's the derivative our gradient for loss with\n          respect to the weights. Now, let's take a look at what's called a\n          single layered neural network. We want to find the weights of the\n          neural network that will minimize the loss of our data set. We compute\n          the gradient of our loss with respect to each of the weights in our\n          neural network. We want to learn a model that accurately describes our\n          test data, not the training data, even though we're optimizing this\n          model based on the training data. This is a very simple neural network\n          that only has one input, one hidden neuron, and one output.\n        </Text>\n        <Text>{length}</Text>\n      </Textbox>\n    </ModalFrame>\n  );\n};\n\nexport default SummaryModal;\n"]},"metadata":{},"sourceType":"module"}